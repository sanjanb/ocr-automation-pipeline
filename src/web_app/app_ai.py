#!/usr/bin/env python3\n\"\"\"\nUpdated FastAPI Web Application with AI-Powered OCR Support\nMIT Hackathon Project\n\nEnhanced web interface supporting both traditional OCR and AI-powered extraction.\n\"\"\"\n\nimport os\nimport sys\nimport uuid\nimport shutil\nimport logging\nfrom pathlib import Path\nfrom typing import List, Optional, Dict, Any\nimport asyncio\nfrom datetime import datetime\n\n# FastAPI imports\nfrom fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Request, Form\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.responses import HTMLResponse, JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nimport uvicorn\n\n# Add the src directory to Python path\ncurrent_dir = Path(__file__).parent\nsrc_dir = current_dir.parent\nsys.path.insert(0, str(src_dir))\n\n# Pipeline imports\nfrom ocr_pipeline import (\n    OCRPipeline, \n    create_pipeline, \n    ProcessingRequest,\n    ProcessingResult,\n    BatchProcessingRequest,\n    BatchProcessingResult,\n    DocumentUpload,\n    ProcessingStatus,\n    APIResponse,\n    DocumentType,\n    OCREngine\n)\n\n# AI Pipeline imports\nfrom ocr_pipeline.enhanced_pipeline import create_enhanced_pipeline, PipelineResult\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize FastAPI app\napp = FastAPI(\n    title=\"OCR Automation Pipeline with AI\",\n    description=\"Document processing pipeline with traditional OCR and AI-powered extraction\",\n    version=\"2.0.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Setup static files and templates\nstatic_dir = current_dir / \"static\"\ntemplates_dir = current_dir / \"templates\"\napp.mount(\"/static\", StaticFiles(directory=str(static_dir)), name=\"static\")\ntemplates = Jinja2Templates(directory=str(templates_dir))\n\n# Global pipeline instances\ntraditional_pipeline = None\nai_pipeline = None\n\n# Upload configuration\nUPLOAD_DIR = current_dir / \"data\" / \"uploads\"\nUPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nALLOWED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\", \".pdf\"}\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Initialize pipelines on startup\"\"\"\n    global traditional_pipeline, ai_pipeline\n    \n    try:\n        logger.info(\"Initializing pipelines...\")\n        \n        # Initialize traditional pipeline\n        traditional_pipeline = create_pipeline()\n        logger.info(\"✅ Traditional OCR pipeline initialized\")\n        \n        # Initialize AI pipeline\n        hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n        ai_enabled = os.getenv(\"AI_EXTRACTION_ENABLED\", \"false\").lower() == \"true\"\n        \n        if ai_enabled or hf_token:\n            ai_pipeline = create_enhanced_pipeline(\n                use_ai=True,\n                hf_token=hf_token,\n                ai_fallback_threshold=float(os.getenv(\"AI_FALLBACK_THRESHOLD\", \"0.6\"))\n            )\n            logger.info(\"✅ AI-powered pipeline initialized\")\n        else:\n            logger.info(\"ℹ️  AI pipeline disabled (no token or setting)\")\n            \n    except Exception as e:\n        logger.error(f\"❌ Pipeline initialization failed: {e}\")\n        # Continue with limited functionality\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    \"\"\"Render the main page with AI extraction options\"\"\"\n    return templates.TemplateResponse(\"index_ai.html\", {\n        \"request\": request,\n        \"ai_enabled\": ai_pipeline is not None,\n        \"hf_token_set\": bool(os.getenv(\"HUGGING_FACE_TOKEN\"))\n    })\n\n@app.post(\"/upload-and-process\")\nasync def upload_and_process(\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...),\n    method: str = Form(\"auto\"),  # \"traditional\", \"ai\", or \"auto\"\n    document_type_hint: Optional[str] = Form(None)\n):\n    \"\"\"\n    Upload and process document with specified extraction method\n    \"\"\"\n    try:\n        # Validate file\n        if not file.filename:\n            raise HTTPException(status_code=400, detail=\"No file provided\")\n            \n        file_ext = Path(file.filename).suffix.lower()\n        if file_ext not in ALLOWED_EXTENSIONS:\n            raise HTTPException(\n                status_code=400, \n                detail=f\"File type {file_ext} not supported. Allowed: {', '.join(ALLOWED_EXTENSIONS)}\"\n            )\n        \n        # Check file size\n        file_content = await file.read()\n        if len(file_content) > MAX_FILE_SIZE:\n            raise HTTPException(\n                status_code=413, \n                detail=f\"File too large. Maximum size: {MAX_FILE_SIZE // 1024 // 1024}MB\"\n            )\n        \n        # Save uploaded file\n        file_id = str(uuid.uuid4())\n        file_path = UPLOAD_DIR / f\"{file_id}_{file.filename}\"\n        \n        with open(file_path, \"wb\") as f:\n            f.write(file_content)\n        \n        logger.info(f\"File uploaded: {file.filename} ({len(file_content)} bytes)\")\n        \n        # Process based on method\n        if method == \"ai\" and ai_pipeline:\n            result = ai_pipeline.process_document(\n                str(file_path), \n                method=\"ai\",\n                document_type_hint=document_type_hint\n            )\n        elif method == \"auto\" and ai_pipeline:\n            result = ai_pipeline.process_document(\n                str(file_path), \n                method=\"auto\",\n                document_type_hint=document_type_hint\n            )\n        else:\n            # Use traditional pipeline\n            if ai_pipeline:\n                result = ai_pipeline.process_document(\n                    str(file_path), \n                    method=\"traditional\",\n                    document_type_hint=document_type_hint\n                )\n            else:\n                # Fallback to old pipeline\n                request = ProcessingRequest(\n                    document_id=file_id,\n                    file_path=str(file_path),\n                    file_name=file.filename,\n                    document_type=document_type_hint,\n                    processing_options={\"method\": \"traditional\"}\n                )\n                result = await traditional_pipeline.process_document(request)\n        \n        # Convert result format if needed\n        if isinstance(result, PipelineResult):\n            response_data = {\n                \"success\": result.success,\n                \"file_id\": file_id,\n                \"filename\": file.filename,\n                \"document_type\": result.document_type.value if hasattr(result.document_type, 'value') else str(result.document_type),\n                \"method_used\": result.method_used,\n                \"confidence\": result.confidence,\n                \"processing_time\": result.processing_time,\n                \"entities\": result.entities or {},\n                \"extracted_text\": result.extracted_text[:500] + \"...\" if len(result.extracted_text) > 500 else result.extracted_text,\n                \"metadata\": result.metadata or {},\n                \"error\": result.error\n            }\n        else:\n            # Handle traditional pipeline result\n            response_data = {\n                \"success\": result.success,\n                \"file_id\": file_id,\n                \"filename\": file.filename,\n                \"document_type\": result.document_type.value if hasattr(result.document_type, 'value') else str(result.document_type),\n                \"method_used\": \"traditional\",\n                \"confidence\": result.confidence,\n                \"processing_time\": getattr(result, 'processing_time', 0),\n                \"entities\": result.extracted_entities or {},\n                \"extracted_text\": result.raw_text[:500] + \"...\" if len(result.raw_text) > 500 else result.raw_text,\n                \"metadata\": result.metadata or {},\n                \"error\": result.error_message or \"\"\n            }\n        \n        # Schedule cleanup\n        background_tasks.add_task(cleanup_file, file_path, delay=300)  # 5 minutes\n        \n        return JSONResponse(content=response_data)\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Processing failed: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Processing failed: {str(e)}\")\n\n@app.get(\"/methods\")\nasync def get_available_methods():\n    \"\"\"Get available extraction methods\"\"\"\n    methods = [\n        {\n            \"id\": \"traditional\",\n            \"name\": \"Traditional OCR\",\n            \"description\": \"Uses OCR engines (EasyOCR, Tesseract) with regex pattern matching\",\n            \"available\": True\n        }\n    ]\n    \n    if ai_pipeline:\n        methods.extend([\n            {\n                \"id\": \"ai\",\n                \"name\": \"AI-Powered Extraction\",\n                \"description\": \"Uses Hugging Face document understanding models (Donut, LayoutLM)\",\n                \"available\": bool(os.getenv(\"HUGGING_FACE_TOKEN\"))\n            },\n            {\n                \"id\": \"auto\", \n                \"name\": \"Auto (AI + Traditional Fallback)\",\n                \"description\": \"Tries AI first, falls back to traditional OCR if confidence is low\",\n                \"available\": bool(os.getenv(\"HUGGING_FACE_TOKEN\"))\n            }\n        ])\n    \n    return JSONResponse(content={\"methods\": methods})\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"traditional_pipeline\": traditional_pipeline is not None,\n        \"ai_pipeline\": ai_pipeline is not None,\n        \"ai_token_configured\": bool(os.getenv(\"HUGGING_FACE_TOKEN\"))\n    }\n\n@app.get(\"/demo\")\nasync def demo_page(request: Request):\n    \"\"\"Demo page with sample documents\"\"\"\n    # Get sample documents\n    samples_dir = current_dir.parent.parent / \"data\" / \"sample_documents\"\n    sample_docs = []\n    \n    if samples_dir.exists():\n        for file_path in samples_dir.glob(\"*\"):\n            if file_path.suffix.lower() in ALLOWED_EXTENSIONS:\n                sample_docs.append({\n                    \"name\": file_path.name,\n                    \"path\": str(file_path),\n                    \"type\": \"marksheet\" if \"MC\" in file_path.name else \"document\"\n                })\n    \n    return templates.TemplateResponse(\"demo_ai.html\", {\n        \"request\": request,\n        \"sample_docs\": sample_docs,\n        \"ai_enabled\": ai_pipeline is not None\n    })\n\nasync def cleanup_file(file_path: Path, delay: int = 300):\n    \"\"\"Clean up uploaded file after delay\"\"\"\n    await asyncio.sleep(delay)\n    try:\n        if file_path.exists():\n            file_path.unlink()\n            logger.info(f\"Cleaned up file: {file_path}\")\n    except Exception as e:\n        logger.error(f\"Failed to cleanup file {file_path}: {e}\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"app_ai:app\",\n        host=\"0.0.0.0\",\n        port=8001,\n        reload=True,\n        log_level=\"info\"\n    )